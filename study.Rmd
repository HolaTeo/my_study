---
title: "Study for SIR"
author: "Choi TaeYoung"
header-includes:
  - \usepackage{kotex}
  - \usepackage{fontspec}
  - \usepackage{unicode-math}
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  pdf_document: 
    latex_engine: xelatex
  bookdown::pdf_document2:
    latex_engine: xelatex
mainfont: NanumGothic
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(tidyverse)
library(dplyr)

library(data.table)

library(dr)
library(textir)
```


## 차원축소 분석 예시(SIR)
```{r}
library(dr)
data(ais)
attach(ais)  # the Australian athletes data
#fit dimension reduction using sir
m1 <- dr(LBM~Wt+Ht+RCC+WCC, method="sir", nslices = 8)
summary(m1)

# repeat, using save:

m2 <- update(m1,method="save")
summary(m2)

# repeat, using phd:

m3 <- update(m2, method="phdres")
summary(m3)


summary(s0 <- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+log(Hc)+log(Ferr), data=ais, slice.function=dr.slices.arc, nslices=8, chi2approx="wood", numdir=4, method="sir"))


## 카테고리 그룹을 포함(pool 옵션의 디폴트값이 FALSE이다. 뜻은 합동분산 추정관련)
summary(s1 <- update(s0, group=~Sex))
```



```{r}
data(we8there)

dim(we8thereCounts)

dim(we8thereRatings)

as.matrix(we8thereCounts)[12,400] ## count for bigram 400 in review 12

##12th review에서 bigram >0 뽑아내기
mm <- as.matrix(we8thereCounts)[12] 
which(mm>0)

## get to know what’s in the matrix
g1 <- min(as.matrix(we8thereCounts)[,]) ## min count over reviews/bigrams
g2 <- max(as.matrix(we8thereCounts)[,]) ## max count over reviews/bigrams 
g1
g2 ## a certain bigram was mentioned in a certain review 13 times

## here we look at the frequencies of the bigram in column 1000 
## the data are extremely sparce
hh <- as.matrix(we8thereCounts)[,1000]


## overall rating 
overall <- as.matrix(we8thereRatings[,1:5])
summary(we8thereRatings[,1:5])
```


```{r eval=FALSE, include=FALSE}
## this will take some time 
nn <- 2640 
cowords <- dim(nn) 
for (i in 1:nn) {
  cowords[i]=sum(as.matrix(we8thereCounts)[,i]) #find the total sum of each different bigram 
  } 
cowords[7]

plot(sort(cowords,decreasing=TRUE)) #제일 빈도수 많은 순서대로 plot그리기
```

```{r eval=FALSE, include=FALSE}
## analysis per review
## we determine the frequencies of bigrams per review 
## this will take some time 
nn <- 6166 
coreview <- dim(nn) 
for (i in 1:nn) { coreview[i]=sum(as.matrix(we8thereCounts)[i,]) 
} 
coreview[5]
plot(sort(coreview,decreasing=TRUE))
```

```{r}
## Multinomial logistic regression and fitted reduction 
## we8mnlm=mnlm(we8thereCounts,overall,bins=5) 
## bins: for faster inference if covariates are factors
## covariate is a factor with 5 levels 
cl <- NULL 
we8mnlm <- mnlm(cl,covars=overall,counts=we8thereCounts,bins=5)
## 여기 mnlm으로 얻는 값들의 구조가 뭔가? 우리가 흔히 하는 coefficient 추정값들 

## we8mnlm$intercept 
## estimates of alphas 
## we8mnlm$loadings 
## estimates of betas 
# fitted(we8mnlm)
# as.matrix(fitted(we8mnlm))[1,]
## fitted counts for first review
## extract coefficients 
B <- coef(we8mnlm) ##2 x 2640 sparse Matrix
B[1,1:10] ## estimates of alpha
B[2,1:10] ## estimates of beta
mean(B[2,]==0) ## sparsity in loadings

## some big loadings in IR 
order(B[2,][1:5]) #2 3 4 1 5(제일 작은 값이 2번째 있고 제일 큰 값이 5번째에 있음)

B[2,order(-B[2,])[1:10]] #제일 큰 coef 1~10번째

## following provides fitted multinomial probabilities
pred <- predict(we8mnlm,overall,type="response")
pred_1 <- pred[1,] ## predicted multinomial probs for review 1 
sum(pred[1,]) ## must add to one

## following predicts inverse prediction (fitted reduction)
## predinv=predict(we8mnlm,we8thereCounts,type="reduction")
predinve <- srproj(B,we8thereCounts) #MNIR projection onto factors 


## 여기 srproj 함수를 뜯어내서 

predinv=predinve[,1] 
predinv[1:10] ## prints predicted ratings for first 10 reviews
```


```{r}
# NOT RUN {
library(MASS)
data(fgl)

## make your cluster 
## FORK is faster but memory heavy, and doesn't work on windows.
cl <- makeCluster(2,type=ifelse(.Platform$OS.type=="unix","FORK","PSOCK")) 
print(cl)

## fit in parallel
fits <- dmr(cl, fgl[,1:9], fgl$type, verb=1)
## 첫함수 <- cl이 군집들
## 둘째 <- fgl들이 값들(상관계수?(covariate))
## 셋째 <- 타입(여기선 2개)


## its good practice stop the cluster once you're done
stopCluster(cl)

## Individual Poisson model fits and AICc selection
par(mfrow=c(3,2))
for(j in 1:6){
	plot(fits[[j]])
	mtext(names(fits)[j],font=2,line=2) }

##  AICc model selection
B <- coef(fits)

## Fitted probability by true response
par(mfrow=c(1,1))
P <- predict(B, fgl[,1:9], type="response")
boxplot(P[cbind(1:214,fgl$type)]~fgl$type, 
	ylab="fitted prob of true class")


# }
```
 
## SIR 적용

- $x_i$가 텍스트로 되어 있는 문서 -> sparse matrix 
- 종속변수 $y_i$를 예측 -> $v_i(=y_i)$에서 $\Phi$라는 계수(coef)를 추정.
- 여기서 $i$는 관측값의 수
- 로지스틱 회귀를 사용가능(역 조건부 분포 활용)
- $y_i$에 대한 정보를 보존 -> 저차원 문서 점수(SR score)를 얻는 역회귀 제안
\[ x_i \sim MN(m_i,q_i)\; \texttt{with}\; q_{ij}=\frac{\exp(\eta_{ij})}{\sum_{i=1^{p}\exp(\eta_{ij})}}, \; j=1,\dots,p
    \]
- where $\eta_{ij}=\alpha_j+u_{ij}+v_i^T \varphi_j$.
- 여기서 $\alpha_j \sim N(0,1)$라고 가정한다.
- 여기서 $\varphi_{jk}$는 독립 라플라스 사전분포를 갖는다. 즉, $\pi(\varphi_{jk})=\lambda_{jk}/2\exp(-\lambda_{jk}|\varphi_{jk}|)$ $\texttt{for}\; j = 1, \dots, p \;\texttt{and} \; k = 1, \dots, K$
- 여기서 내포된 $\varphi_{jk}$의 사전 표준편차는 $\sqrt{2}/\lambda_{jk}$
- 여기서 각 $\lambda_{jk}$에 할당된 공액 감마 초모수의 분포는 $\texttt{Gamma}(\lambda_{jk};s,r)=r^s/\Gamma(s)\lambda^{s-1}_{jk}e^{-r\lambda_{jk}}$
- 여기서 $s$은 shape parameter, $r$은 rate parameter, 평균은 $s/r$, 분산은 $s/r^2$이다.

- 결국 우리가 관심있는 사후 분포는 아래와 같다.
\[p(\mathbf{\Phi},\mathbf{\alpha},\mathbf{\lambda},\mathbf{U}|\mathbf{X},\mathbf{V})\propto \prod_{i=1}^n \prod_{j=0}^p q_{ij}^{x_{ij}}\pi(u_{ij})N(\alpha_j;0,\sigma^2_{\alpha})\prod^K_{k=1}\text{GL}(\varphi_{jk},\lambda_{jk})
\]

- 여기서 감마-라쏘의 사후분포를 $c(\varphi_{jk})=s\log(1+|\varphi_{jk}|/r)$로 나타낼 수 있다.

- 이 모든 것을 로그를 취하고, 음수를 취한 뒤, 상수항을 제거한 식은 $l(\alpha_j,\varphi_j)+\sum^p_{j=1}(\alpha_j/\sigma_{\alpha})^2+c(\Phi)$ 인 이 식을 최소화하는 $\hat{\alpha}, \hat{\varphi}$를 찾으면 된다.

- 여기서 $l(\alpha_j,\varphi_j)=\sum^n_{i=1}[m_ie^{\alpha_j+v_i^T\varphi_j}-x_{ij}(\alpha_j+v_i^T\varphi_j)]$
```{r}
x.i <- as.matrix(we8thereCounts)
v.i <- as.matrix(we8thereRatings)

s <- 1
r <- .5


```
 

1. SIR coefficient들 찾기(알파, 베타?) ex) mnir에서는 6x2640 차원 스파스 매트릭스 만들어짐
2. 그 값들을 메트릭스로 곂치는 차원으로 만들기 ex) 19개 컬럼이름
3. x corv 메트릭스 생성 -> 충분차원 축소 상황 발생
4. SR 스코어?로 해석하는 건지